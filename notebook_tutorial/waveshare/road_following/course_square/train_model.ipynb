{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Road Follower - Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this notebook we will train a neural network to take an input image, and output a set of x, y values corresponding to a target.\n",
    "\n",
    "We will be using PyTorch deep learning framework to train ResNet18 neural network architecture model for road follower application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このノートブックでは、入力画像を受け取り、ターゲットに対応するx, y値のセットを出力するニューラルネットワークを学習します。\n",
    "\n",
    "PyTorch深層学習フレームワークを使用して、ResNet18ニューラルネットワークアーキテクチャモデルをロードフォロワーアプリケーション用に訓練する予定です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import glob\n",
    "import PIL.Image\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and extract data\n",
    "\n",
    "Before you start, you should upload the ``road_following_<Date&Time>.zip`` file that you created in the ``data_collection.ipynb`` notebook on the robot. \n",
    "\n",
    "> If you're training on the JetBot you collected data on, you can skip this!\n",
    "\n",
    "You should then extract this dataset by calling the command below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ↓のコマンドをターミナルから入力(\"!\"は要らない). ファイル名は\"road_following_<タイムスタンプの値>.zip\"になっているので,ファイル名を直す事.\n",
    "# !unzip -q road_following.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a folder named ``dataset_all`` appear in the file browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset Instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a custom ``torch.utils.data.Dataset`` implementation, which implements the ``__len__`` and ``__getitem__`` functions.  This class\n",
    "is responsible for loading images and parsing the x, y values from the image filenames.  Because we implement the ``torch.utils.data.Dataset`` class,\n",
    "we can use all of the torch data utilities :)\n",
    "\n",
    "We hard coded some transformations (like color jitter) into our dataset.  We made random horizontal flips optional (in case you want to follow a non-symmetric path, like a road\n",
    "where we need to 'stay right').  If it doesn't matter whether your robot follows some convention, you could enable flips to augment the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここでは、カスタム ``torch.utils.data.Dataset`` の実装を作成し、 ``__len__`` と ``__getitem__`` 関数を実装しています。 このクラスは画像の読み込みと、画像ファイル名から x, y 値をパースする役割を担っています。 ここでは、 ``torch.utils.data.Dataset`` クラスを実装しているので、torch.utils.data.Dataset`` のすべての機能を利用することができます。\n",
    "\n",
    "データセットにいくつかの変換をハードコードしています（色ずれなど）。 ランダムな水平方向の反転はオプションにしました。\n",
    "水平方向のランダムフリップはオプションとしました（道路のように左右対称でない経路をたどる場合、「右に寄る」必要があるため）。 もし、あなたのロボットが何らかの慣習に従っているかどうかが重要でないなら、データセットを増やすために反転を有効にすることができる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x(path):\n",
    "    \"\"\"Gets the x value from the image filename\"\"\"\n",
    "    return (float(int(path[3:6])) - 50.0) / 50.0\n",
    "\n",
    "def get_y(path):\n",
    "    \"\"\"Gets the y value from the image filename\"\"\"\n",
    "    return (float(int(path[7:10])) - 50.0) / 50.0\n",
    "\n",
    "class XYDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, directory, random_hflips=False):\n",
    "        self.directory = directory\n",
    "        self.random_hflips = random_hflips\n",
    "        self.image_paths = glob.glob(os.path.join(self.directory, '*.jpg'))\n",
    "        self.color_jitter = transforms.ColorJitter(0.3, 0.3, 0.3, 0.3)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        \n",
    "        image = PIL.Image.open(image_path)\n",
    "        x = float(get_x(os.path.basename(image_path)))\n",
    "        y = float(get_y(os.path.basename(image_path)))\n",
    "        \n",
    "        if float(np.random.rand(1)) > 0.5:\n",
    "            image = transforms.functional.hflip(image)\n",
    "            x = -x\n",
    "        \n",
    "        image = self.color_jitter(image)\n",
    "        image = transforms.functional.resize(image, (224, 224))\n",
    "        image = transforms.functional.to_tensor(image)\n",
    "        image = image.numpy()[::-1].copy()\n",
    "        image = torch.from_numpy(image)\n",
    "        image = transforms.functional.normalize(image, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        \n",
    "        return image, torch.tensor([x, y]).float()\n",
    "    \n",
    "dataset = XYDataset('dataset_course_square', random_hflips=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset into train and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Once we read dataset, we will split data set in train and test sets. In this example we split train and test a 90%-10%. The test set will be used to verify the accuracy of the model we train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データセットを読み込んだら、トレーニングセットとテストセットに分割します。  \n",
    "この例では、トレーニングセットとテストセットを90%-10%に分割しています。テストセットは、学習したモデルの精度を検証するために使用されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_percent = 0.1\n",
    "num_test = int(test_percent * len(dataset))\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [len(dataset) - num_test, num_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data loaders to load data in batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We use ``DataLoader`` class to load data in batches, shuffle data and allow using multi-subprocesses. In this example we use batch size of 64. Batch size will be based on memory available with your GPU and it can impact accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``DataLoader`` クラスを使用する事でデータからバッチを取り出し、データをシャッフルし、複数のサブプロセスを使用できるようにします。  \n",
    "この例では、64のバッチサイズを使用しています。バッチサイズはGPUで利用可能なメモリに依存し、モデルの精度に影響を与える可能性があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Neural Network Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We use ResNet-18 model available on PyTorch TorchVision. \n",
    "\n",
    "In a process called transfer learning, we can repurpose a pre-trained model (trained on millions of images) for a new task that has possibly much less data available.\n",
    "\n",
    "\n",
    "More details on ResNet-18 : https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n",
    "\n",
    "More Details on Transfer Learning: https://www.youtube.com/watch?v=yofjFQddwHE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここでは、PyTorch TorchVisionで利用可能なResNet-18モデルを使用します。\n",
    "\n",
    "転移学習と呼ばれるプロセスでは、事前に学習したモデル（数百万枚の画像で学習）を、利用可能なデータがはるかに少ない新しいタスクに再利用することができます。\n",
    "\n",
    "\n",
    "ResNet-18の詳細: https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n",
    "\n",
    "転移学習の詳細: https://www.youtube.com/watch?v=yofjFQddwHE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "ResNet model has fully connect (fc) final layer with 512 as ``in_features`` and we will be training for regression thus ``out_features`` as 1\n",
    "\n",
    "Finally, we transfer our model for execution on the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNetのモデルは最終層がfc(fully connect)で512がin_featuresで、回帰学習を行うのでout_featuresが1になる。\n",
    "\n",
    "最後に、GPUで実行するためにモデルを転送します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = torch.nn.Linear(512, 2)\n",
    "device = torch.device('cuda')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "\n",
    "We train for 70 epochs and save best model if the loss is reduced. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``エポック数=70``で学習させ、損失が減少した場合に最適なモデルを保存する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:0.514412, 9.616317\n",
      "0.083614, 0.634283\n",
      "0.049146, 0.160356\n",
      "0.041641, 0.138066\n",
      "0.040379, 0.014028\n",
      "0.030088, 0.029166\n",
      "0.043515, 0.034930\n",
      "0.023821, 0.017119\n",
      "0.022245, 0.014232\n",
      "0.018284, 0.015713\n",
      "10:0.027353, 0.021361\n",
      "0.016658, 0.013999\n",
      "0.015816, 0.010554\n",
      "0.018898, 0.008317\n",
      "0.014283, 0.016182\n",
      "0.014256, 0.017886\n",
      "0.015383, 0.015167\n",
      "0.014568, 0.006548\n",
      "0.018318, 0.024219\n",
      "0.014985, 0.015034\n",
      "20:0.012297, 0.016174\n",
      "0.021948, 0.015867\n",
      "0.021306, 0.016391\n",
      "0.016974, 0.006744\n",
      "0.020464, 0.007809\n",
      "0.023047, 0.014876\n",
      "0.016593, 0.013702\n",
      "0.016858, 0.007393\n",
      "0.016989, 0.011721\n",
      "0.014616, 0.014481\n",
      "30:0.009067, 0.019405\n",
      "0.011241, 0.020674\n",
      "0.013871, 0.021096\n",
      "0.022581, 0.029193\n",
      "0.022239, 0.022932\n",
      "0.015773, 0.021686\n",
      "0.014110, 0.010976\n",
      "0.010375, 0.021737\n",
      "0.021287, 0.010650\n",
      "0.012310, 0.013949\n",
      "40:0.011816, 0.008231\n",
      "0.008969, 0.011640\n",
      "0.008632, 0.007420\n",
      "0.009168, 0.009218\n",
      "0.009983, 0.023633\n",
      "0.010823, 0.009258\n",
      "0.007488, 0.012175\n",
      "0.005740, 0.008690\n",
      "0.007342, 0.008211\n",
      "0.005174, 0.006737\n",
      "50:0.005142, 0.015478\n",
      "0.007933, 0.013590\n",
      "0.012142, 0.012243\n",
      "0.014314, 0.005824\n",
      "0.008731, 0.006697\n",
      "0.004820, 0.005680\n",
      "0.004217, 0.012824\n",
      "0.006062, 0.007654\n",
      "0.008912, 0.008719\n",
      "0.006251, 0.023116\n",
      "60:0.009420, 0.008904\n",
      "0.005315, 0.004936\n",
      "0.004352, 0.004826\n",
      "0.004801, 0.009308\n",
      "0.004263, 0.005193\n",
      "0.002989, 0.014746\n",
      "0.005015, 0.007974\n",
      "0.003749, 0.018517\n",
      "0.003999, 0.005229\n",
      "0.006408, 0.028819\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'printf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-63a3c3db2f6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%f, %f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# printf(\"%f,%f\",train_loss,test_loss)と同じ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m69\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mprintf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training finished.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtest_loss\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'printf' is not defined"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 70\n",
    "BEST_MODEL_PATH = 'best_steering_model_square.pth'\n",
    "best_loss = 1e9\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for images, labels in iter(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = F.mse_loss(outputs, labels)\n",
    "        train_loss += float(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_loss /= len(train_loader)\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    for images, labels in iter(test_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = F.mse_loss(outputs, labels)\n",
    "        test_loss += float(loss)\n",
    "    test_loss /= len(test_loader)\n",
    "    \n",
    "    # 学習の進歩の表示\n",
    "    if (epoch % 10) == 0:\n",
    "        print('%d:' %(epoch),end='')  # 10の倍数のときに表示してわかりやすく\n",
    "    \n",
    "    print('%f, %f' % (train_loss, test_loss))  # printf(\"%f,%f\",train_loss,test_loss)と同じ\n",
    "    if (epoch==69):\n",
    "        print('training finished.')\n",
    "    \n",
    "    if test_loss < best_loss:\n",
    "        torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
    "        best_loss = test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Once the model is trained, it will generate ``best_steering_model_xy.pth`` file which you can use for inferencing in the live demo notebook.\n",
    "\n",
    "If you trained on a different machine other than JetBot, you'll need to upload this to the JetBot to the ``road_following`` example folder."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
