{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ライントレース+物体検知"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/usr/lib/python36.zip', '/usr/lib/python3.6', '/usr/lib/python3.6/lib-dynload', '', '/home/jetbot/.local/lib/python3.6/site-packages', '/usr/local/lib/python3.6/dist-packages', '/usr/local/lib/python3.6/dist-packages/torchvision-0.4.0a0+d31eafa-py3.6-linux-aarch64.egg', '/usr/local/lib/python3.6/dist-packages/Adafruit_SSD1306-1.6.2-py3.6.egg', '/usr/local/lib/python3.6/dist-packages/Adafruit_MotorHAT-1.4.0-py3.6.egg', '/usr/local/lib/python3.6/dist-packages/Adafruit_GPIO-1.0.4-py3.6.egg', '/usr/local/lib/python3.6/dist-packages/spidev-3.4-py3.6-linux-aarch64.egg', '/usr/local/lib/python3.6/dist-packages/Adafruit_PureIO-1.0.4-py3.6.egg', '/usr/local/lib/python3.6/dist-packages/jetbot-0.4.0-py3.6.egg', '/usr/lib/python3/dist-packages', '/usr/lib/python3.6/dist-packages', '/usr/local/lib/python3.6/dist-packages/IPython/extensions', '/home/jetbot/.ipython', '/home/jetbot/g031r066/darknet']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append( \"/home/jetbot/g031r066/darknet\" )\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 物体検知側の下準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import darknet\n",
    "import darknet_images\n",
    "import time\n",
    "\"\"\"\n",
    "load model description and weights from config files\n",
    "args:\n",
    "    config_file (str): path to .cfg model file\n",
    "    data_file (str): path to .data model file\n",
    "    weights (str): path to weights\n",
    "returns:\n",
    "    network: trained model\n",
    "    class_names\n",
    "    class_colors\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "network, class_names, class_colors = darknet.load_network(\n",
    "    '/home/jetbot/g031r066/darknet/cfg/yolov4-tiny.cfg',  \n",
    "    '/home/jetbot/g031r066/darknet/cfg/coco.data', \n",
    "    '/home/jetbot/g031r066/darknet/weights/yolov4-tiny.weights'\n",
    "    )\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "network, class_names, class_colors = darknet.load_network(\n",
    "        '/home/jetbot/g031r066/darknet/custom/yolov4-tiny-custom.cfg', \n",
    "        '/home/jetbot/g031r066/darknet/custom/custom.data',  \n",
    "        '/home/jetbot/g031r066/darknet/custom/backup/yolov4-tiny-custom_final.weights'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ライントレース側の下準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# インポート\n",
    "import torchvision\n",
    "import torch\n",
    "\n",
    "# 自分で作成したライントレースのモデルを読み込む\n",
    "model_rf = torchvision.models.resnet18(pretrained=False)\n",
    "model_rf.fc = torch.nn.Linear(512, 2)\n",
    "model_rf.load_state_dict(torch.load('/home/jetbot/g031r066/note_nvidia/road_following/best_steering_models/[nvidia_circuit-ver1.2]_res18.pth'))\n",
    "# GPU側に転送\n",
    "device = torch.device('cuda')\n",
    "model_rf = model_rf.to(device)\n",
    "model_rf = model_rf.eval().half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# カメラから読み込む映像への前処理を実装\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "\n",
    "mean = torch.Tensor([0.485, 0.456, 0.406]).cuda().half()\n",
    "std = torch.Tensor([0.229, 0.224, 0.225]).cuda().half()\n",
    "\n",
    "def preprocess(image):\n",
    "    image = PIL.Image.fromarray(image)\n",
    "    image = transforms.functional.to_tensor(image).to(device).half()\n",
    "    image.sub_(mean[:, None, None]).div_(std[:, None, None])\n",
    "    return image[None, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 諸々インポート\n",
    "from IPython.display import display\n",
    "import ipywidgets\n",
    "import traitlets\n",
    "\n",
    "# 駆動系周りの準備\n",
    "from jetbot import Robot\n",
    "\n",
    "robot = Robot()  # 駆動系を制御できるモジュールのインポート\n",
    "\n",
    "# 制御するバーの設定\n",
    "speed_gain_slider = ipywidgets.FloatSlider(min=0.0, max=1.0, step=0.01, description='speed gain')\n",
    "steering_gain_slider = ipywidgets.FloatSlider(min=0.0, max=1.0, step=0.01, value=0.05, description='steering gain')\n",
    "steering_dgain_slider = ipywidgets.FloatSlider(min=0.0, max=0.5, step=0.001, value=0.0, description='steering kd')\n",
    "steering_bias_slider = ipywidgets.FloatSlider(min=-0.3, max=0.3, step=0.01, value=0.0, description='steering bias')\n",
    "# 描画\n",
    "#display(speed_gain_slider, steering_gain_slider, steering_dgain_slider, steering_bias_slider)\n",
    "\n",
    "# 自Jetbotにおいては、以下の設定だと良い感じに動く\n",
    "# speed gain = 0.1 2022/09/09 0.15の方が綺麗\n",
    "# steering gain = 0.05\n",
    "# steering kd = 0\n",
    "# steering gain = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 現在の動作を示すバーを描画\n",
    "x_slider = ipywidgets.FloatSlider(min=-1.0, max=1.0, description='x')\n",
    "y_slider = ipywidgets.FloatSlider(min=0, max=1.0, orientation='vertical', description='y')\n",
    "steering_slider = ipywidgets.FloatSlider(min=-1.0, max=1.0, description='steering')\n",
    "speed_slider = ipywidgets.FloatSlider(min=0, max=1.0, orientation='vertical', description='speed')\n",
    "\n",
    "#display(ipywidgets.HBox([y_slider, speed_slider]))\n",
    "#display(x_slider, steering_slider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 本処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetbot import Camera, bgr8_to_jpeg\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "# カメラ起動\n",
    "camera = Camera(width=300, height=300, capture_width=1280, capture_height=720)  # デフォルトでは Camera(width=224, height=224, fps=21, capture_width=3280, capture_height=2464)\n",
    "image_widget = ipywidgets.Image()\n",
    "traitlets.dlink((camera, 'value'), (image_widget, 'value'), transform=bgr8_to_jpeg)\n",
    "\n",
    "blocked_widget = ipywidgets.FloatSlider(min=0.0, max=1.0, value=0.0, description='blocked')\n",
    "image_widget = ipywidgets.Image(format='jpeg', width=300, height=300)\n",
    "bbox_widget = ipywidgets.Image(format='jpeg', width=300, height=300)\n",
    "label_widget = ipywidgets.IntText(value=1, description='tracked label')\n",
    "speed_widget = ipywidgets.FloatSlider(value=0.4, min=0.0, max=1.0, description='speed')\n",
    "turn_gain_widget = ipywidgets.FloatSlider(value=0.8, min=0.0, max=2.0, description='turn gain')\n",
    "'''\n",
    "display(ipywidgets.VBox([\n",
    "    ipywidgets.HBox([image_widget, blocked_widget),\n",
    "    label_widget,\n",
    "    speed_widget,\n",
    "    turn_gain_widget\n",
    "]))\n",
    "'''\n",
    "width = int(image_widget.width)\n",
    "height = int(image_widget.height)\n",
    "\n",
    "def detection_center(detection):\n",
    "    \"\"\"Computes the center x, y coordinates of the object\"\"\"\n",
    "    bbox = detection['bbox']\n",
    "    center_x = (bbox[0] + bbox[2]) / 2.0 - 0.5\n",
    "    center_y = (bbox[1] + bbox[3]) / 2.0 - 0.5\n",
    "    return (center_x, center_y)\n",
    "    \n",
    "def norm(vec):\n",
    "    \"\"\"Computes the length of the 2D vector\"\"\"\n",
    "    return np.sqrt(vec[0]**2 + vec[1]**2)\n",
    "\n",
    "def closest_detection(detections):\n",
    "    \"\"\"Finds the detection closest to the image center\"\"\"\n",
    "    closest_detection = None\n",
    "    for det in detections:\n",
    "        center = detection_center(det)\n",
    "        if closest_detection is None:\n",
    "            closest_detection = det\n",
    "        elif norm(detection_center(det)) < norm(detection_center(closest_detection)):\n",
    "            closest_detection = det\n",
    "    return closest_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "angle = 0.0\n",
    "angle_last = 0.0\n",
    "# ライントレースを実装する関数\n",
    "def road_following(image):\n",
    "    global angle, angle_last\n",
    "\n",
    "    xy = model_rf(preprocess(image)).detach().float().cpu().numpy().flatten()\n",
    "    x = xy[0]\n",
    "    y = (0.5 - xy[1]) / 2.0\n",
    "    \n",
    "    x_slider.value = x\n",
    "    y_slider.value = y\n",
    "    \n",
    "    speed_slider.value = speed_gain_slider.value\n",
    "    \n",
    "    angle = np.arctan2(x, y)\n",
    "    pid = angle * steering_gain_slider.value + (angle - angle_last) * steering_dgain_slider.value\n",
    "    angle_last = angle\n",
    "    \n",
    "    steering_slider.value = pid + steering_bias_slider.value\n",
    "    \n",
    "    robot.left_motor.value = max(min(speed_slider.value + steering_slider.value, 1.0), 0.0)\n",
    "    robot.right_motor.value = max(min(speed_slider.value - steering_slider.value, 1.0), 0.0)\n",
    "\n",
    "    return 0\n",
    "\n",
    "# 物体検知を実装する関数\n",
    "def object_detection(snapshot):\n",
    "    \n",
    "    re = 0\n",
    "    prev_time = time.time()\n",
    "    thresh = .45\n",
    "    image, detections = darknet_images.image_detection(snapshot, network, class_names, class_colors, thresh)\n",
    "    darknet.print_detections(detections, True)  # True: 各物体の座標・幅・高さを表示\n",
    "    fps = int(1/(time.time() - prev_time))\n",
    "    print(\"FPS: {}\".format(fps), flush=True)\n",
    "    print(\"Predicted in {} seconds\".format((time.time() - prev_time)), flush=True)\n",
    "    \n",
    "    label_list=[]\n",
    "    confidence_list=[]\n",
    "    bbox_list=[]\n",
    "    for label, confidence, bbox in detections: \n",
    "        label_list.append(label)\n",
    "        confidence_list.append(confidence)\n",
    "        bbox_list.append(bbox)\n",
    "    \n",
    "    if(\"Traffic light\" in label_list):  # 信号機を検知した場合\n",
    "        # 対象の信号機を1つに絞る\n",
    "        trafficlight_label_list=[]\n",
    "        trafficlight_confidence_list=[]\n",
    "        trafficlight_bbox_list=[]\n",
    "        for label, confidence, bbox in detections: \n",
    "            if(label==\"Traffic light\"):\n",
    "                trafficlight_label_list.append(label)\n",
    "                trafficlight_confidence_list.append(confidence)\n",
    "                trafficlight_bbox_list.append(bbox)\n",
    "        \n",
    "        target_index = trafficlight_confidence_list.index(max(trafficlight_confidence_list))  # 検知した信号機のうち、1番信頼度の高いものを対象にし、そのindexを取ってくる\n",
    "        target_bbox =  trafficlight_bbox_list[target_index]  # 取ってきたindexを基に、1版信頼度の高い信号機の座標を保存\n",
    "        print(\"target_bbox: {}\".format(target_bbox), flush=True)\n",
    "        \n",
    "        xmin, ymin, xmax, ymax = darknet.bbox2points(target_bbox)\n",
    "        print(type(image))\n",
    "        #cropped_image = image[ymin+1:ymax-1, xmin+1:xmax-1]  # bboxの枠線が映らないようにちょっと狭めに切り取る\n",
    "        cropped_image = image[ymin:ymax, xmin:xmax] \n",
    "        \n",
    "        # 1.HSVに変換して白黒でマスク(2値化)\n",
    "        # cropped_imageが空になっていることがある、要処理追加\n",
    "        hsv_cropped_image = cv2.cvtColor(cropped_image, cv2.COLOR_BGR2HSV)\n",
    "        # 信号機の目:白,それ以外:黒 でマスクする \n",
    "        # 彩度(s)・明度(v)で絞り込む. 信号機の目は大体鮮やかなのでそのようになるように指定\n",
    "        lower = np.array([0, 200,100])           # 抽出する色の下限(h,s,v)\n",
    "        upper = np.array([150, 255 , 255])        # 抽出する色の上限(h,s,v)\n",
    "        mask_traffic_light = cv2.inRange(hsv_cropped_image, lower, upper) # inRangeで元画像を２値化\n",
    "        # 1の結果を基に、信号機の目部分の色はそのままに、それ以外を黒でマスク\n",
    "        target = cv2.bitwise_and(hsv_cropped_image,hsv_cropped_image, mask=mask_traffic_light)\n",
    "        # 信号機の目以外は黒色(H=0)でマスクしている為、Hの平均値を出しても小さい値にしかならないので意味がない\n",
    "        # → なのでHのminとmaxを出す(min要らないけど)\n",
    "        h_min = target.T[0].flatten().min()\n",
    "        h_max = target.T[0].flatten().max()\n",
    "        \n",
    "        # Hの最大値で判別する\n",
    "        if(0 <= h_max <= 30):\n",
    "            print(\"Traffic_light : red\", flush=True)\n",
    "            # 赤信号なので一時停止\n",
    "            robot.left_motor.value = 0.0\n",
    "            robot.right_motor.value = 0.0\n",
    "            re = -1\n",
    "            time.sleep(0.1) \n",
    "        elif(30<h_max and h_max<=150):  # 緑と青は取り敢えず一緒にした\n",
    "            print(\"Traffic_light : green\", flush=True)\n",
    "            re = 0\n",
    "        else:\n",
    "            print(\"Traffic_light : none\", flush=True)\n",
    "            re = -2\n",
    "    else:  # 信号機を検知できなかった場合\n",
    "        now = datetime.datetime.now().isoformat(sep=' ', timespec='milliseconds')\n",
    "        filename = '/home/jetbot/g031r066/darknet/bad_snapshot/' + str(now) + '.jpg'\n",
    "        #cv2.imwrite(filename, snapshot)  # 信号機を検知できなかったときのスナップショットを保存\n",
    "    \n",
    "    clear_output(True)\n",
    "            \n",
    "    return re, image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "def execute(change):\n",
    "    snapshot = change['new']  # 映像読み込み type(snapshot) = numpy.ndarray\n",
    "    # メモリがカツカツなせいか実行開始からしばらくするとカメラからのソースが途切れるので、その際の処理\n",
    "    if(snapshot.size == 0):  # カメラからのソースが途切れたら\n",
    "        robot.left_motor.value = 0.0  # ロボットを一時停止させる\n",
    "        robot.right_motor.value = 0.0\n",
    "        camera.restart()  # カメラ再起動\n",
    "        time.sleep(0.1)  # 一応チャタリング防止\n",
    "        bbox_widget.value = bgr8_to_jpeg(camera.value.copy)\n",
    "    else:  # カメラからのソースを正常に取得できている場合\n",
    "        re, image = object_detection(snapshot)  # 物体検知を実行する\n",
    "        if(re == -1):  # 赤信号時\n",
    "            print(\"road_following don't work.(re == -1)\")\n",
    "        else:\n",
    "            road_following(snapshot)  # 赤信号ではないときのみ、ライントレースを行う\n",
    "            print(\"road_following is work.\")\n",
    "        \n",
    "        # 出力されているカメラ映像の更新\n",
    "        bbox_widget.value = bgr8_to_jpeg(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c23d82d4f63d4cee8e0a9a8e0f1afa5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\xff\\xdb\\x00C…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import HBox, VBox\n",
    "\n",
    "display(HBox([bbox_widget, VBox([speed_gain_slider, steering_gain_slider, steering_dgain_slider, steering_bias_slider])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "road_following is work.\n",
      "\n",
      "Objects:\n",
      "Traffic light: 73.2%    (left_x: 171   top_y:  53   width:   40   height:  121)\n",
      "Traffic sign: 80.51%    (left_x: 171   top_y:  53   width:   40   height:  121)\n",
      "FPS: 10\n",
      "Predicted in 0.09511542320251465 seconds\n",
      "target_bbox: (171.2945098876953, 52.94718933105469, 39.62138366699219, 121.39842987060547)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/jetbot-0.4.0-py3.6.egg/jetbot/camera.py\", line 45, in _capture_frames\n",
      "    self.value = image\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/traitlets/traitlets.py\", line 588, in __set__\n",
      "    self.set(obj, value)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/traitlets/traitlets.py\", line 577, in set\n",
      "    obj._notify_trait(self.name, old_value, new_value)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/traitlets/traitlets.py\", line 1210, in _notify_trait\n",
      "    type='change',\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/traitlets/traitlets.py\", line 1215, in notify_change\n",
      "    return self._notify_observers(change)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/traitlets/traitlets.py\", line 1252, in _notify_observers\n",
      "    c(event)\n",
      "  File \"<ipython-input-8-6e60deefdec5>\", line 13, in execute\n",
      "    re, image = object_detection(snapshot)  # 物体検知を実行する\n",
      "  File \"<ipython-input-13-fadc2b64b897>\", line 68, in object_detection\n",
      "    hsv_cropped_image = cv2.cvtColor(cropped_image, cv2.COLOR_BGR2HSV)\n",
      "cv2.error: OpenCV(4.1.1) /home/nvidia/host/build_opencv/nv_opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "execute({'new': camera.value})\n",
    "camera.observe(execute, names='value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.unobserve(execute, names='value')\n",
    "\n",
    "time.sleep(0.1)  # add a small sleep to make sure frames have finished processing\n",
    "camera.stop()\n",
    "robot.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.stop()\n",
    "robot.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.restart()\n",
    "time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
