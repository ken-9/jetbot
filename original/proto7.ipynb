{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ライントレース+物体検知"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/usr/lib/python36.zip', '/usr/lib/python3.6', '/usr/lib/python3.6/lib-dynload', '', '/home/jetbot/.local/lib/python3.6/site-packages', '/usr/local/lib/python3.6/dist-packages', '/usr/local/lib/python3.6/dist-packages/torchvision-0.4.0a0+d31eafa-py3.6-linux-aarch64.egg', '/usr/local/lib/python3.6/dist-packages/Adafruit_SSD1306-1.6.2-py3.6.egg', '/usr/local/lib/python3.6/dist-packages/Adafruit_MotorHAT-1.4.0-py3.6.egg', '/usr/local/lib/python3.6/dist-packages/Adafruit_GPIO-1.0.4-py3.6.egg', '/usr/local/lib/python3.6/dist-packages/spidev-3.4-py3.6-linux-aarch64.egg', '/usr/local/lib/python3.6/dist-packages/Adafruit_PureIO-1.0.4-py3.6.egg', '/usr/local/lib/python3.6/dist-packages/jetbot-0.4.0-py3.6.egg', '/usr/lib/python3/dist-packages', '/usr/lib/python3.6/dist-packages', '/usr/local/lib/python3.6/dist-packages/IPython/extensions', '/home/jetbot/.ipython', '/home/jetbot/g031r066/darknet']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append( \"/home/jetbot/g031r066/darknet\" )\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 物体検知側の下準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import darknet\n",
    "import darknet_images\n",
    "import time\n",
    "\"\"\"\n",
    "load model description and weights from config files\n",
    "args:\n",
    "    config_file (str): path to .cfg model file\n",
    "    data_file (str): path to .data model file\n",
    "    weights (str): path to weights\n",
    "returns:\n",
    "    network: trained model\n",
    "    class_names\n",
    "    class_colors\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "network, class_names, class_colors = darknet.load_network(\n",
    "    '/home/jetbot/g031r066/darknet/cfg/yolov4-tiny.cfg',  \n",
    "    '/home/jetbot/g031r066/darknet/cfg/coco.data', \n",
    "    '/home/jetbot/g031r066/darknet/weights/yolov4-tiny.weights'\n",
    "    )\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "network, class_names, class_colors = darknet.load_network(\n",
    "        '/home/jetbot/g031r066/darknet/custom/yolov4-tiny-custom.cfg', \n",
    "        '/home/jetbot/g031r066/darknet/custom/custom.data',  \n",
    "        '/home/jetbot/g031r066/darknet/custom/backup/yolov4-tiny-custom_final.weights'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ライントレース側の下準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# インポート\n",
    "import torchvision\n",
    "import torch\n",
    "\n",
    "# 自分で作成したライントレースのモデルを読み込む\n",
    "model_rf = torchvision.models.resnet18(pretrained=False)\n",
    "model_rf.fc = torch.nn.Linear(512, 2)\n",
    "model_rf.load_state_dict(torch.load('/home/jetbot/g031r066/note_nvidia/road_following/best_steering_models/[nvidia_circuit-ver1.2]_res18.pth'))\n",
    "# GPU側に転送\n",
    "device = torch.device('cuda')\n",
    "model_rf = model_rf.to(device)\n",
    "model_rf = model_rf.eval().half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# カメラから読み込む映像への前処理を実装\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "\n",
    "mean = torch.Tensor([0.485, 0.456, 0.406]).cuda().half()\n",
    "std = torch.Tensor([0.229, 0.224, 0.225]).cuda().half()\n",
    "\n",
    "def preprocess(image):\n",
    "    image = PIL.Image.fromarray(image)\n",
    "    image = transforms.functional.to_tensor(image).to(device).half()\n",
    "    image.sub_(mean[:, None, None]).div_(std[:, None, None])\n",
    "    return image[None, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 諸々インポート\n",
    "from IPython.display import display\n",
    "import ipywidgets\n",
    "import traitlets\n",
    "\n",
    "# 駆動系周りの準備\n",
    "from jetbot import Robot\n",
    "\n",
    "robot = Robot()  # 駆動系を制御できるモジュールのインポート\n",
    "\n",
    "# 制御するバーの設定\n",
    "speed_gain_slider = ipywidgets.FloatSlider(min=0.0, max=1.0, step=0.01, description='speed gain')\n",
    "steering_gain_slider = ipywidgets.FloatSlider(min=0.0, max=1.0, step=0.01, value=0.05, description='steering gain')\n",
    "steering_dgain_slider = ipywidgets.FloatSlider(min=0.0, max=0.5, step=0.001, value=0.0, description='steering kd')\n",
    "steering_bias_slider = ipywidgets.FloatSlider(min=-0.3, max=0.3, step=0.01, value=0.0, description='steering bias')\n",
    "# 描画\n",
    "#display(speed_gain_slider, steering_gain_slider, steering_dgain_slider, steering_bias_slider)\n",
    "\n",
    "# 自Jetbotにおいては、以下の設定だと良い感じに動く\n",
    "# speed gain = 0.1 2022/09/09 0.15の方が綺麗\n",
    "# steering gain = 0.05\n",
    "# steering kd = 0\n",
    "# steering gain = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 現在の動作を示すバーを描画\n",
    "x_slider = ipywidgets.FloatSlider(min=-1.0, max=1.0, description='x')\n",
    "y_slider = ipywidgets.FloatSlider(min=0, max=1.0, orientation='vertical', description='y')\n",
    "steering_slider = ipywidgets.FloatSlider(min=-1.0, max=1.0, description='steering')\n",
    "speed_slider = ipywidgets.FloatSlider(min=0, max=1.0, orientation='vertical', description='speed')\n",
    "\n",
    "#display(ipywidgets.HBox([y_slider, speed_slider]))\n",
    "#display(x_slider, steering_slider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 本処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetbot import Camera, bgr8_to_jpeg\n",
    "import datetime\n",
    "import time\n",
    "import queue\n",
    "\n",
    "# カメラ起動\n",
    "camera = Camera(width=300, height=300, capture_width=1280, capture_height=720)  # デフォルトでは Camera(width=224, height=224, fps=21, capture_width=3280, capture_height=2464)\n",
    "image_widget = ipywidgets.Image()\n",
    "traitlets.dlink((camera, 'value'), (image_widget, 'value'), transform=bgr8_to_jpeg)\n",
    "\n",
    "# メインスレッド<->サブスレッド で画像をやり取りするためのキューを作っておく\n",
    "queue_rf = queue.Queue()  # road_following()用\n",
    "queue_od = queue.Queue()  # object_detection()用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocked_widget = ipywidgets.FloatSlider(min=0.0, max=1.0, value=0.0, description='blocked')\n",
    "image_widget = ipywidgets.Image(format='jpeg', width=300, height=300)\n",
    "bbox_widget = ipywidgets.Image(format='jpeg', width=300, height=300)\n",
    "label_widget = ipywidgets.IntText(value=1, description='tracked label')\n",
    "speed_widget = ipywidgets.FloatSlider(value=0.4, min=0.0, max=1.0, description='speed')\n",
    "turn_gain_widget = ipywidgets.FloatSlider(value=0.8, min=0.0, max=2.0, description='turn gain')\n",
    "'''\n",
    "display(ipywidgets.VBox([\n",
    "    ipywidgets.HBox([image_widget, blocked_widget),\n",
    "    label_widget,\n",
    "    speed_widget,\n",
    "    turn_gain_widget\n",
    "]))\n",
    "'''\n",
    "width = int(image_widget.width)\n",
    "height = int(image_widget.height)\n",
    "\n",
    "def detection_center(detection):\n",
    "    \"\"\"Computes the center x, y coordinates of the object\"\"\"\n",
    "    bbox = detection['bbox']\n",
    "    center_x = (bbox[0] + bbox[2]) / 2.0 - 0.5\n",
    "    center_y = (bbox[1] + bbox[3]) / 2.0 - 0.5\n",
    "    return (center_x, center_y)\n",
    "    \n",
    "def norm(vec):\n",
    "    \"\"\"Computes the length of the 2D vector\"\"\"\n",
    "    return np.sqrt(vec[0]**2 + vec[1]**2)\n",
    "\n",
    "def closest_detection(detections):\n",
    "    \"\"\"Finds the detection closest to the image center\"\"\"\n",
    "    closest_detection = None\n",
    "    for det in detections:\n",
    "        center = detection_center(det)\n",
    "        if closest_detection is None:\n",
    "            closest_detection = det\n",
    "        elif norm(detection_center(det)) < norm(detection_center(closest_detection)):\n",
    "            closest_detection = det\n",
    "    return closest_detection\n",
    "\n",
    "angle = 0.0\n",
    "angle_last = 0.0\n",
    "\n",
    "# ライントレースを実装する関数\n",
    "def road_following(snapshot):\n",
    "    global angle, angle_last\n",
    "\n",
    "    xy = model_rf(preprocess(snapshot)).detach().float().cpu().numpy().flatten()\n",
    "    x = xy[0]\n",
    "    y = (0.5 - xy[1]) / 2.0\n",
    "    \n",
    "    x_slider.value = x\n",
    "    y_slider.value = y\n",
    "    \n",
    "    speed_slider.value = speed_gain_slider.value\n",
    "    \n",
    "    angle = np.arctan2(x, y)\n",
    "    pid = angle * steering_gain_slider.value + (angle - angle_last) * steering_dgain_slider.value\n",
    "    angle_last = angle\n",
    "    \n",
    "    steering_slider.value = pid + steering_bias_slider.value\n",
    "    \n",
    "    left_motor_rf = max(min(speed_slider.value + steering_slider.value, 1.0), 0.0)\n",
    "    right_motor_rf = max(min(speed_slider.value - steering_slider.value, 1.0), 0.0)\n",
    "\n",
    "    queue_rf.put(left_motor_rf)\n",
    "    queue_rf.put(right_motor_rf)\n",
    "\n",
    "# 物体検知を実装する関数\n",
    "def object_detection(snapshot):\n",
    "    \n",
    "    re = 0\n",
    "    prev_time = time.time()\n",
    "    thresh = .45\n",
    "    image, detections = darknet_images.image_detection(snapshot, network, class_names, class_colors, thresh)\n",
    "    darknet.print_detections(detections, True)  # True: 各物体の座標・幅・高さを表示\n",
    "    fps = int(1/(time.time() - prev_time))\n",
    "    print(\"FPS: {}\".format(fps), flush=True)\n",
    "    print(\"Predicted in {} seconds\".format((time.time() - prev_time)), flush=True)\n",
    "    \n",
    "    label_list=[]\n",
    "    confidence_list=[]\n",
    "    bbox_list=[]\n",
    "    for label, confidence, bbox in detections: \n",
    "        label_list.append(label)\n",
    "        confidence_list.append(confidence)\n",
    "        bbox_list.append(bbox)\n",
    "    \n",
    "    queue_od.put(image)  # メインスレッドにbboxを描画した画像を送る\n",
    "    \n",
    "    if(\"Traffic light\" in label_list):  # 信号機部分を切り出し\n",
    "        # 対象の信号機を1つに絞る\n",
    "        trafficlight_label_list=[]\n",
    "        trafficlight_confidence_list=[]\n",
    "        trafficlight_bbox_list=[]\n",
    "        for label, confidence, bbox in detections: \n",
    "            if(label==\"Traffic light\"):\n",
    "                trafficlight_label_list.append(label)\n",
    "                trafficlight_confidence_list.append(confidence)\n",
    "                trafficlight_bbox_list.append(bbox)\n",
    "        \n",
    "        target_index = trafficlight_confidence_list.index(max(trafficlight_confidence_list))  # 検知した信号機のうち、1番信頼度の高いものを対象にし、そのindexを取ってくる\n",
    "        target_bbox =  trafficlight_bbox_list[target_index]  # 取ってきたindexを基に、1版信頼度の高い信号機の座標を保存       \n",
    "        xmin, ymin, xmax, ymax = darknet.bbox2points(target_bbox)\n",
    "        xmin = xmin-1\n",
    "        ymin = ymin-1\n",
    "        xmax = xmax-1\n",
    "        ymax = ymax-1\n",
    "        print(\"xmin:{} ymin:{}, xmax:{}, ymax:{}\".format(xmin, ymin, xmax, ymax), flush=True)\n",
    "        # クロッピングする際、座標が負の値だとエラーになるので該当する変数は0にする\n",
    "        if(xmin<0):\n",
    "            xmin = 0\n",
    "        elif(ymin<0):\n",
    "            ymin = 0\n",
    "        elif(xmax<0):\n",
    "            xmax = 0\n",
    "        elif(ymax<0):\n",
    "            ymax = 0\n",
    "            \n",
    "        cropped_image = image[ymin:ymax, xmin:xmax]  # bboxの枠線が映らないようにちょっと狭めに切り取る\n",
    "        \n",
    "        #import numpy as np\n",
    "        # 1.HSVに変換して白黒でマスク(2値化)\n",
    "        hsv_cropped_image = cv2.cvtColor(cropped_image, cv2.COLOR_BGR2HSV)\n",
    "        # 信号機の目:白,それ以外:黒 でマスクする \n",
    "        # 彩度(s)・明度(v)で絞り込む. 信号機の目は大体鮮やかなのでそのようになるように指定\n",
    "        lower = np.array([0, 200,100])           # 抽出する色の下限(h,s,v)\n",
    "        upper = np.array([150, 255 , 255])        # 抽出する色の上限(h,s,v)\n",
    "        mask_traffic_light = cv2.inRange(hsv_cropped_image, lower, upper) # inRangeで元画像を２値化\n",
    "        \n",
    "        # 1の結果を基に、信号機の目部分の色はそのままに、それ以外を黒でマスク\n",
    "        target = cv2.bitwise_and(hsv_cropped_image,hsv_cropped_image, mask=mask_traffic_light)\n",
    "        \n",
    "        # 信号機の目以外は黒色(H=0)でマスクしている為、Hの平均値を出しても小さい値にしかならないので意味がない\n",
    "        # → なのでHのminとmaxを出す(min要らないけど)\n",
    "        h_min = target.T[0].flatten().min()\n",
    "        h_max = target.T[0].flatten().max()\n",
    "        #print(\"h_min: %s\" % (str(h_min)))\n",
    "        #print(\"h_max: %s\" % (str(h_max)))\n",
    "        \n",
    "        # Hの最大値で判別する\n",
    "        if(0 <= h_max <= 30):\n",
    "            re = -1\n",
    "            print(\"Traffic_light : red\", flush=True)\n",
    "        elif(30<h_max and h_max<=150):  # 緑と青は取り敢えず一緒にした\n",
    "            re = 0\n",
    "            print(\"Traffic_light : green\", flush=True)\n",
    "        else:\n",
    "            re = -1\n",
    "            print(\"Traffic_light : none\", flush=True)\n",
    "    else:\n",
    "        now = datetime.datetime.now().isoformat(sep=' ', timespec='milliseconds')\n",
    "        #filename = '/home/jetbot/g031r066/darknet/bad_snapshot/' + str(now) + '.jpg'\n",
    "        #cv2.imwrite(filename, snapshot)  # 信号機を検知できなかったときのスナップショットを保存\n",
    "    \n",
    "    clear_output(True)\n",
    "    queue_od.put(re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def execute(change):\n",
    "    snapshot = change['new']  # 映像読み込み type(snapshot) = numpy.ndarray\n",
    "\n",
    "    # ライントレースと物体検知のスレッドをそれぞれ作成\n",
    "    thread_rf = threading.Thread(target = road_following, args=(snapshot,))\n",
    "    thread_od = threading.Thread(target = object_detection, args=(snapshot,))\n",
    "    \n",
    "    # カクカクになるけどちゃんと動いてくれるのでは?\n",
    "    robot.left_motor.value = 0.0\n",
    "    robot.right_motor.value = 0.0\n",
    "    \n",
    "    # 各スレッド実行開始\n",
    "    thread_rf.start()\n",
    "    thread_od.start()\n",
    "\n",
    "    # 各スレッド内の処理が終わるまで待つ\n",
    "    thread_rf.join()\n",
    "    thread_od.join()\n",
    "    \n",
    "    # 各スレッド内での結果を受け取る\n",
    "    left_motor_rf = queue_rf.get()   # ライントレース実行時における右タイヤの制御値\n",
    "    right_motor_rf = queue_rf.get()  # ライントレース実行時における左タイヤの制御値\n",
    "    image = queue_od.get()  # 物体検知実行時のbboxが描画された画像\n",
    "    re = queue_od.get()  # 物体検知実行時の返り値\n",
    "    \n",
    "    if(re == -1):  # 赤信号時\n",
    "        # 一時停止\n",
    "        robot.left_motor.value = 0.0\n",
    "        robot.right_motor.value = 0.0\n",
    "        time.sleep(0.1)\n",
    "    else:  # 赤信号以外\n",
    "        robot.left_motor.value = left_motor_rf\n",
    "        robot.right_motor.value = right_motor_rf\n",
    "\n",
    "    # 出力されているカメラ映像の更新\n",
    "    #image_widget.value = bgr8_to_jpeg(snapshot)\n",
    "    bbox_widget.value = bgr8_to_jpeg(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb07034921524f43ac75c11b8c4051b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'', format='jpeg', height='300', width='300'), VBox(children=(FloatSlider(value=0.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import HBox, VBox\n",
    "\n",
    "#display(HBox([image_widget,bbox_widget]))\n",
    "display(HBox([bbox_widget, VBox([speed_gain_slider, steering_gain_slider, steering_dgain_slider, steering_bias_slider])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Objects:\n",
      "Traffic sign: 74.25%    (left_x: 394   top_y:  378   width:   43   height:  64)\n",
      "FPS: 4\n",
      "Predicted in 0.2286677360534668 seconds\n"
     ]
    }
   ],
   "source": [
    "execute({'new': camera.value})\n",
    "camera.observe(execute, names='value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.unobserve(execute, names='value')\n",
    "\n",
    "time.sleep(0.1)  # add a small sleep to make sure frames have finished processing\n",
    "camera.stop()\n",
    "robot.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.stop()\n",
    "robot.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "robot.left_motor.value = 0.0\n",
    "robot.right_motor.value = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.restart()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
