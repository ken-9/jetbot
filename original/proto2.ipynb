{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ライントレース+物体検知"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 物体検知側の下準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetbot import ObjectDetector\n",
    "\n",
    "# 物体検知モデルの読み込み\n",
    "model_od = ObjectDetector('../note_wavashare/object_following/ssd_mobilenet_v2_coco.engine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetbot import Camera\n",
    "\n",
    "camera = Camera.instance(width=300, height=300)  # カメラの起動"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "mean_obj = 255.0 * np.array([0.485, 0.456, 0.406])\n",
    "stdev_obj = 255.0 * np.array([0.229, 0.224, 0.225])\n",
    "normalize_obj = torchvision.transforms.Normalize(mean_obj, stdev_obj)\n",
    "\n",
    "def preprocess_obj(camera_value):\n",
    "    global device, normalize_obj\n",
    "    x = camera_value\n",
    "    x = cv2.resize(x, (224, 224))\n",
    "    x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
    "    x = x.transpose((2, 0, 1))\n",
    "    x = torch.from_numpy(x).float()\n",
    "    x = normalize(x)\n",
    "    x = x.to(device)\n",
    "    x = x[None, ...]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ライントレース側の下準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# インポート\n",
    "import torchvision\n",
    "import torch\n",
    "\n",
    "# 自分で作成したライントレースのモデルを読み込む\n",
    "model_rf = torchvision.models.resnet18(pretrained=False)\n",
    "model_rf.fc = torch.nn.Linear(512, 2)\n",
    "model_rf.load_state_dict(torch.load('../note_nvidia/road_following/best_steering_models/[nvidia_circuit-ver1.2]_res18.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU側に転送\n",
    "device = torch.device('cuda')\n",
    "model_rf = model_rf.to(device)\n",
    "model_rf = model_rf.eval().half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# カメラから読み込む映像への前処理を実装\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "\n",
    "mean = torch.Tensor([0.485, 0.456, 0.406]).cuda().half()\n",
    "std = torch.Tensor([0.229, 0.224, 0.225]).cuda().half()\n",
    "\n",
    "def preprocess(image):\n",
    "    image = PIL.Image.fromarray(image)\n",
    "    image = transforms.functional.to_tensor(image).to(device).half()\n",
    "    image.sub_(mean[:, None, None]).div_(std[:, None, None])\n",
    "    return image[None, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# カメラからの映像をセルの下に出力\n",
    "from IPython.display import display\n",
    "import ipywidgets\n",
    "import traitlets\n",
    "from jetbot import Camera, bgr8_to_jpeg\n",
    "\n",
    "# camera = Camera()\n",
    "# image_widget = ipywidgets.Image()\n",
    "# traitlets.dlink((camera, 'value'), (image_widget, 'value'), transform=bgr8_to_jpeg)\n",
    "\n",
    "# display(image_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e4c40e67cee43ee8497f646c38edaa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.0, description='speed gain', max=1.0, step=0.01)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e8f85fbf0554c20909fffb1560536ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.05, description='steering gain', max=1.0, step=0.01)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14eca15609cb4bafa2b8ec2c1cc40d51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.0, description='steering kd', max=0.5, step=0.001)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a53a35cbca7d4c568c8314e03d65082d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.0, description='steering bias', max=0.3, min=-0.3, step=0.01)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 駆動系周りの準備\n",
    "from jetbot import Robot\n",
    "\n",
    "robot = Robot()  # 駆動系を制御できるモジュールのインポート\n",
    "\n",
    "# 制御するバーの設定\n",
    "speed_gain_slider = ipywidgets.FloatSlider(min=0.0, max=1.0, step=0.01, description='speed gain')\n",
    "steering_gain_slider = ipywidgets.FloatSlider(min=0.0, max=1.0, step=0.01, value=0.05, description='steering gain')\n",
    "steering_dgain_slider = ipywidgets.FloatSlider(min=0.0, max=0.5, step=0.001, value=0.0, description='steering kd')\n",
    "steering_bias_slider = ipywidgets.FloatSlider(min=-0.3, max=0.3, step=0.01, value=0.0, description='steering bias')\n",
    "\n",
    "# 描画\n",
    "display(speed_gain_slider, steering_gain_slider, steering_dgain_slider, steering_bias_slider)\n",
    "\n",
    "# 自Jetbotにおいては、以下の設定だと良い感じに動く\n",
    "# speed gain = 0.1 2022/09/09 0.15の方が綺麗\n",
    "# steering gain = 0.05\n",
    "# steering kd = 0\n",
    "# steering gain = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d8b2121b10f4e22a749fec01af01c57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatSlider(value=0.0, description='y', max=1.0, orientation='vertical'), FloatSlider(value=0.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "870ac6bdee5f4d4eaf67c12c66d8f573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.0, description='x', max=1.0, min=-1.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2c6fe7cf1ae45b3b6a625c5c68382e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.0, description='steering', max=1.0, min=-1.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 現在の動作を示すバーを描画\n",
    "x_slider = ipywidgets.FloatSlider(min=-1.0, max=1.0, description='x')\n",
    "y_slider = ipywidgets.FloatSlider(min=0, max=1.0, orientation='vertical', description='y')\n",
    "steering_slider = ipywidgets.FloatSlider(min=-1.0, max=1.0, description='steering')\n",
    "speed_slider = ipywidgets.FloatSlider(min=0, max=1.0, orientation='vertical', description='speed')\n",
    "\n",
    "display(ipywidgets.HBox([y_slider, speed_slider]))\n",
    "display(x_slider, steering_slider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 本処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9ac78179110468b8acb3a5df07c1219",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Image(value=b'', format='jpeg', height='300', width='300'), FloatSlider(value=0.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from jetbot import bgr8_to_jpeg\n",
    "import time\n",
    "\n",
    "blocked_widget = ipywidgets.FloatSlider(min=0.0, max=1.0, value=0.0, description='blocked')\n",
    "image_widget = ipywidgets.Image(format='jpeg', width=300, height=300)\n",
    "label_widget = ipywidgets.IntText(value=1, description='tracked label')\n",
    "speed_widget = ipywidgets.FloatSlider(value=0.4, min=0.0, max=1.0, description='speed')\n",
    "turn_gain_widget = ipywidgets.FloatSlider(value=0.8, min=0.0, max=2.0, description='turn gain')\n",
    "\n",
    "display(ipywidgets.VBox([\n",
    "    ipywidgets.HBox([image_widget, blocked_widget]),\n",
    "    label_widget,\n",
    "    speed_widget,\n",
    "    turn_gain_widget\n",
    "]))\n",
    "\n",
    "width = int(image_widget.width)\n",
    "height = int(image_widget.height)\n",
    "\n",
    "def detection_center(detection):\n",
    "    \"\"\"Computes the center x, y coordinates of the object\"\"\"\n",
    "    bbox = detection['bbox']\n",
    "    center_x = (bbox[0] + bbox[2]) / 2.0 - 0.5\n",
    "    center_y = (bbox[1] + bbox[3]) / 2.0 - 0.5\n",
    "    return (center_x, center_y)\n",
    "    \n",
    "def norm(vec):\n",
    "    \"\"\"Computes the length of the 2D vector\"\"\"\n",
    "    return np.sqrt(vec[0]**2 + vec[1]**2)\n",
    "\n",
    "def closest_detection(detections):\n",
    "    \"\"\"Finds the detection closest to the image center\"\"\"\n",
    "    closest_detection = None\n",
    "    for det in detections:\n",
    "        center = detection_center(det)\n",
    "        if closest_detection is None:\n",
    "            closest_detection = det\n",
    "        elif norm(detection_center(det)) < norm(detection_center(closest_detection)):\n",
    "            closest_detection = det\n",
    "    return closest_detection\n",
    "\n",
    "angle = 0.0\n",
    "angle_last = 0.0\n",
    "\n",
    "# ライントレースを実装する関数\n",
    "def road_following(image):\n",
    "    global angle, angle_last\n",
    "\n",
    "    xy = model_rf(preprocess(image)).detach().float().cpu().numpy().flatten()\n",
    "    x = xy[0]\n",
    "    y = (0.5 - xy[1]) / 2.0\n",
    "    \n",
    "    x_slider.value = x\n",
    "    y_slider.value = y\n",
    "    \n",
    "    speed_slider.value = speed_gain_slider.value\n",
    "    \n",
    "    angle = np.arctan2(x, y)\n",
    "    pid = angle * steering_gain_slider.value + (angle - angle_last) * steering_dgain_slider.value\n",
    "    angle_last = angle\n",
    "    \n",
    "    steering_slider.value = pid + steering_bias_slider.value\n",
    "    \n",
    "    robot.left_motor.value = max(min(speed_slider.value + steering_slider.value, 1.0), 0.0)\n",
    "    robot.right_motor.value = max(min(speed_slider.value - steering_slider.value, 1.0), 0.0)\n",
    "\n",
    "    return 0\n",
    "\n",
    "# 物体検知を実装する関数\n",
    "def object_detection(image):\n",
    "    # カメラの映像内において、検知出来た全ての物体を、変数detectionに入れる\n",
    "    detections = model_od(image)\n",
    "    \n",
    "    # 出力されているカメラ映像の上に、物体の枠(バウンディングボックス)を描画する\n",
    "    label_id=[]\n",
    "    for det in detections[0]:\n",
    "        # バウンディングボックスの描画\n",
    "        bbox = det['bbox']\n",
    "        image = cv2.rectangle(image, (int(width * bbox[0]), int(height * bbox[1])), (int(width * bbox[2]), int(height * bbox[3])), (255, 0, 0), 2)\n",
    "        \n",
    "        # ラベルIDを表示\n",
    "        # label_id = det['label'] と記述してしまうと、det['label']が空の場合、label_idは宣言されない扱いになってしまうので、Noneが代入された配列として宣言する\n",
    "        label_id.append(str(det['label']))\n",
    "        \n",
    "        # {この辺に、「ラベルIDと一致する名前を持ってくる」みたいな処理}\n",
    "        cv2.putText(image, \"id:\"+str(det['label']), (int(width * bbox[0]), int(height * bbox[1]+3)), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (36,255,12), 2)\n",
    "        \n",
    "    # print(label_id)\n",
    "    \n",
    "    # select detections that match selected class label\n",
    "      # 追跡する物体を設定する(ユーザがlabel番号を指定、その番号をmatching_detectionsに代入)\n",
    "      # label_widget.value : ユーザが指定するやつ、1とか2とか設定できるやつ\n",
    "      # d['label'] : \n",
    "    matching_detections = [d for d in detections[0] if d['label'] == int(label_widget.value)]\n",
    "    \n",
    "    # get detection closest to center of field of view and draw it\n",
    "    det = closest_detection(matching_detections)\n",
    "    if det is not None:\n",
    "        bbox = det['bbox']\n",
    "        cv2.rectangle(image, (int(width * bbox[0]), int(height * bbox[1])), (int(width * bbox[2]), int(height * bbox[3])), (0, 255, 0), 5)\n",
    "    \n",
    "    # 検知した物体によって動きを変える\n",
    "    pause_flag = str(44) in label_id  # bottleを検知したら一時停止\n",
    "    if(pause_flag == True):\n",
    "        print(\"pause\")\n",
    "        robot.stop()\n",
    "        time.sleep(0.1) \n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pause\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "def execute(change):\n",
    "    # start_time = time.time()\n",
    "    \n",
    "    image = change['new']  # 映像読み込み\n",
    "\n",
    "    # ライントレースと物体検知のスレッドをそれぞれ作成\n",
    "    thread_rf = threading.Thread(target = road_following, args=(image,))\n",
    "    thread_od = threading.Thread(target = object_detection, args=(image,))\n",
    "\n",
    "    # 各スレッド実行開始\n",
    "    thread_rf.start()\n",
    "    thread_od.start()\n",
    "\n",
    "    # 各スレッド内の処理が終わるまで待つ\n",
    "    thread_rf.join()\n",
    "    thread_od.join()\n",
    "\n",
    "    # 出力されているカメラ映像の更新\n",
    "    image_widget.value = bgr8_to_jpeg(image)\n",
    "    \n",
    "    # finish_time = time.time()\n",
    "    # excute_time = finish_time - start_time\n",
    "    # print(excute_time)\n",
    "    \n",
    "execute({'new': camera.value})\n",
    "camera.observe(execute, names='value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! We've created our neural network execution function, but now we need to attach it to the camera for processing.\n",
    "\n",
    "We accomplish that with the observe function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">WARNING: This code will move the robot!! Please make sure your robot has clearance and it is on Lego or Track you have collected data on. The road follower should work, but the neural network is only as good as the data it's trained on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.unobserve(execute, names='value')\n",
    "\n",
    "time.sleep(0.1)  # add a small sleep to make sure frames have finished processing\n",
    "camera.stop()\n",
    "robot.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.stop()\n",
    "robot.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
